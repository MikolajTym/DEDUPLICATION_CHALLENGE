{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8be3227-c101-4921-94d8-72f9164ee909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T20:38:44.815534Z",
     "start_time": "2023-03-18T20:38:39.342930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Required libraries to run code \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hnswlib\n",
    "from scipy.spatial import distance\n",
    "from datasketch import MinHash, MinHashLSHForest,MinHashLSHEnsemble\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369b4f7-c6e4-4ae8-b1f6-ff7ee30b8373",
   "metadata": {},
   "source": [
    "## Load the main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31fb32a6-e3c7-48cb-b616-4d54b9b00c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T20:38:48.172852Z",
     "start_time": "2023-03-18T20:38:44.815534Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/wi_dataset.csv', lineterminator='\\n', parse_dates=['retrieval_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973e332-f412-4c06-8b0c-1f6363678c1b",
   "metadata": {},
   "source": [
    "# SentenceRemover \n",
    "- SentenceRemover is a collection of methods developed to remove part of offer that occurs many times in others e.g. (text not connected with real job description)\n",
    "- It is based on IDF to measure how many times a 5-gram occurs within job offers for each language.\n",
    "- As a preprocessing step we used some regular expressions and make use of LCS (longest common substring) to detect part of offer that was duplicated inside description to ensure data quality for creation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bfaa0-72ea-4d58-8713-a660fa119710",
   "metadata": {},
   "source": [
    "## Initialize shared methods for Sentence Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7799a64e-2356-4930-98af-4c57157f8037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T20:38:48.192837Z",
     "start_time": "2023-03-18T20:38:48.172852Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_dup_longest_common_substring(text):\n",
    "    \"\"\"Find duplicated the longest common substring(sequence of words) inside text\"\"\"\n",
    "    \"\"\"To speed up computation instead of comparing chars we use entire tokens(words separated by space)\"\"\"\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split(\" \")\n",
    "    n = len(words)\n",
    "    \n",
    "    # Initialize a matrix to store the lengths of longest common suffixes\n",
    "    matrix = [[0] * (n + 1) for _ in range(n + 1)]\n",
    "    \n",
    "    # Initialize variables to store the length and position of the longest common substring\n",
    "    max_length = 0\n",
    "    end_pos = 0\n",
    "    \n",
    "    # Iterate over all possible pairs of suffixes\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(i + 1, n + 1):\n",
    "            \n",
    "            # Check if the suffixes match\n",
    "            if words[i - 1] == words[j - 1]:\n",
    "                # If they do, update the matrix entry and check if it's the longest match so far\n",
    "                matrix[i][j] = matrix[i - 1][j - 1] + 1\n",
    "                if matrix[i][j] > max_length:\n",
    "                    max_length = matrix[i][j]\n",
    "                    end_pos = i\n",
    "            \n",
    "            # If the suffixes don't match, set the matrix entry to 0\n",
    "            else:\n",
    "                matrix[i][j] = 0\n",
    "    \n",
    "    # Use the end position and length of the longest match to return the matching substring\n",
    "    return \" \".join(words[end_pos - max_length:end_pos])\n",
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    \"\"\"The function that will be used(in TfidfVectorizer) to convert the input text into a sequence of tokens \n",
    "    (i.e., words or subwords) that will be used as features in the resulting document-term matrix.\"\"\"\n",
    "    # Remove digits from the text\n",
    "    text = re.sub(r'[\\d]+', '', text)\n",
    "    # Remove punctuation from the text\n",
    "    text = re.sub(r\"\"\"[!\"#$%&\\\\'()*+,\\-./:;<=>?@[\\]^_`{|}~]+\"\"\", ' ', text)\n",
    "    # Remove sequences of the same character (at least 3 in a row)\n",
    "    text = re.sub(r\"(\\w)\\1{2,}\" , \"\", text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Find all word tokens in the text\n",
    "    tokens = re.findall(r\"(?u)\\b\\w\\w+\\b\", text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_duplicate_sentences(text):\n",
    "    \"\"\"Function that takes in the raw input text and applies preprocessing steps to it before tokenizing (in TfidfVectorizer)\"\"\"\n",
    "    text = re.sub(r'\\\\n', '', text)  # Remove newline characters\n",
    "    text = re.sub(r\"\"\"([\\w!\"#$%&\\\\'()*+,\\-./:;<=>?@[\\]^_`{|}~])\\1{2,}\"\"\" , \"\", text)  # Remove many of the same character (at least 3 in a row)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    \n",
    "    sub = find_dup_longest_common_substring(text)  # Find the longest duplicated substring\n",
    "    if len(sub.split()) > 15:  # If the substring is longer than 15 words\n",
    "        text = text.replace(sub, \" \", 1)  # Replace only the first occurrence of the substring with a space\n",
    "    return text  # Return the modified text\n",
    "\n",
    "def buildNgramDictionary(vectorizer,country):\n",
    "    \"\"\"\n",
    "    Function designed to create dictionary which stores the most popular 5-grams to be removed.\n",
    "    \n",
    "    Args:\n",
    "    - vectorizer: a vectorizer object with IDF weights already computed\n",
    "    - country: a string representing the country for which the dictionary is being built\n",
    "    \n",
    "    Returns:\n",
    "    - terms_dict: a dictionary containing the n-grams with IDF values smaller than the chosen quantile\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the feature names and idf values from the vectorizer\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    idf_values = vectorizer.idf_\n",
    "    \n",
    "    # Remove duplicate idf values and sort the array in descending order\n",
    "    unique_idf = list(set(idf_values))\n",
    "    sorted_idf = np.sort(unique_idf)[::-1]\n",
    "    \n",
    "    # Compute the three largest values in the array\n",
    "    largest_values = sorted_idf[:3]\n",
    "    \n",
    "    # Set the initial quantile value to 0.02\n",
    "    q = 0.02\n",
    "    idf_quantile = 0\n",
    "    \n",
    "    # If the job offers are from Luxembourg, set the threshold manually\n",
    "    if country == 'LU':\n",
    "        q = 0.0001\n",
    "    else:\n",
    "        # Compute the quantile value\n",
    "        q_value = np.quantile(idf_values, q)\n",
    "        \n",
    "        # Check if the quantile value is different from the three largest values\n",
    "        if q_value not in largest_values:\n",
    "            idf_quantile = np.quantile(idf_values, q)\n",
    "        else:\n",
    "            #set third highest value\n",
    "            idf_quantile = largest_values[-1]\n",
    "\n",
    "    # Create a dictionary with only the terms that have an idf value smaller than the quantile\n",
    "    terms_dict = {}\n",
    "    for i, term in enumerate(feature_names):\n",
    "        if idf_values[i] <= idf_quantile:\n",
    "            terms_dict[term] = idf_values[i]\n",
    "            \n",
    "    return terms_dict\n",
    "\n",
    "\n",
    "def removeTrashSentencesIDFv2(document, vectorizer_dict):\n",
    "    \"\"\"It takes in two arguments:\n",
    "    1. document - a string representing the text to be cleaned.\n",
    "    2. vectorizer_dict - a dictionary containing for each language the 'TfidfVectorizer' object; \n",
    "    built tfidf_vectorizer.build_preprocessor() and tfidf_vectorizer.build_tokenizer() function; the dictionary with 5-gram to remove.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the document is a string, proceed with the cleaning process.\n",
    "    if(not isinstance(document,str)):\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        if document in short_desc_cache:#is saved copy\n",
    "            return short_desc_cache[document]\n",
    "        else:\n",
    "            # Get the dictionary of 5-grams to be removed.\n",
    "            terms_dict = vectorizer_dict['terms_dict']\n",
    "            removal_ = terms_dict.keys()#5grams to remove\n",
    "            \n",
    "            # Apply the preprocessor function to the document.\n",
    "            document_pre=vectorizer_dict['vectorizerPreprocessor'](document)\n",
    "            \n",
    "          \n",
    "            # Remove identified 5-grams from text using a generator expression and a regular expression.\n",
    "            # The pattern is defined as - (...|ngram|...) where ngram is each 5-gram to be removed and the \n",
    "            # text is every token joined with space.\n",
    "            cleaned_text = re.sub(r'\\b{}\\b'.format('|'.join(ngram for ngram in removal_)), '', ' '.join(word for word in vectorizer_dict['vectorizerTokenizer'](document_pre)))\n",
    "            \n",
    "            # Replace all consecutive whitespaces with a single whitespace.\n",
    "            cleaned_text  =re.sub(r\"\\s+\",\" \",cleaned_text)\n",
    "            \n",
    "            # If the cleaned_text is empty, return the original document.\n",
    "            if cleaned_text==\"\":\n",
    "                cleaned_text = document\n",
    "            \n",
    "            #save short_desc inside dictionary for reuse\n",
    "            short_desc_cache[document] = cleaned_text\n",
    "            # Otherwise, return the cleaned_text.\n",
    "            return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051080cb-27ec-47cf-bdee-530f544058bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make separate vocabulary (of 5-grams to remove) based on IDF for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3d4c30-3653-4501-a80a-dc38e9a821a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T20:38:48.208721Z",
     "start_time": "2023-03-18T20:38:48.195008Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_dict={}#stores vectorizer for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fbb6214-ceec-458c-863a-8aa76cc2437e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T20:38:48.229403Z",
     "start_time": "2023-03-18T20:38:48.213736Z"
    }
   },
   "outputs": [],
   "source": [
    "countryList = df.country_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc7520e-b283-4868-8110-f16e84bd31d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T20:48:04.569012Z",
     "start_time": "2023-03-18T20:38:48.233414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57af9787c89d46b18603270fd4aedfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Iterate through each country in the list of countries\n",
    "for country in tqdm(countryList):\n",
    "    # Create a TfidfVectorizer object with 5-grams, remove duplicate sentences, and custom tokenizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(5, 5), preprocessor=remove_duplicate_sentences, dtype='float32', tokenizer=custom_tokenizer)\n",
    "    \n",
    "    # Filter the dataframe by the current country, drop duplicate descriptions, drop rows with missing descriptions,\n",
    "    # and reset the index\n",
    "    tfIDF = df.query(f\"country_id == '{country}'\").drop_duplicates(['description']).dropna(subset=['description']).reset_index(drop=True)\n",
    "    \n",
    "    # Fit the TfidfVectorizer object to the descriptions of the filtered dataframe\n",
    "    tfidf_vectorizer.fit(tfIDF['description'])\n",
    "    \n",
    "    # Create a dictionary for the current country in the vectorizer_dict\n",
    "    vectorizer_dict[f\"{country}\"] = {}\n",
    "    \n",
    "    # Add the TfidfVectorizer object, preprocessor function, tokenizer function, and n-gram dictionary to the country's dictionary\n",
    "    vectorizer_dict[f\"{country}\"]['vectorizer'] = tfidf_vectorizer\n",
    "    vectorizer_dict[f\"{country}\"]['vectorizerPreprocessor'] = tfidf_vectorizer.build_preprocessor()\n",
    "    vectorizer_dict[f\"{country}\"]['vectorizerTokenizer'] = tfidf_vectorizer.build_tokenizer()\n",
    "    vectorizer_dict[f\"{country}\"]['terms_dict']  = buildNgramDictionary(tfidf_vectorizer, country)\n",
    "    \n",
    "##TIME : 7 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf08fe1-72bd-4cd7-aaa7-96dbb72c970b",
   "metadata": {},
   "source": [
    "## Apply SentenceRemover on entire DF to create short(quality) version of description 'short_desc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6723af70-97cc-4c0c-8d9f-13d79e1a3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_desc_cache={} #dictionary to store 'description':'short_desc' for faster computation of short descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9736fd89-a396-4298-98d1-f5d01a1fff54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4858f999f2487e8d73303a98c399f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['short_desc'] = df.progress_apply(lambda x: removeTrashSentencesIDFv2(x['description'],vectorizer_dict[x['country_id']]), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f27af-04b8-454e-9103-02a915a3a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TIME 5:51H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2a7310-18b1-4345-928e-c03860098a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_desc_cache={}#free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20daa209-2ae3-4f47-b882-debf97713a24",
   "metadata": {},
   "source": [
    "## SentenceTransformer - Generating description embeddings for entire DF\n",
    "- The pretrained model is https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2 with maximum number of input tokens (512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e376bd8e-f390-4d47-9a14-c33ea77283a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max seq length: <bound method SentenceTransformer.get_max_seq_length of SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "#Instatiance SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "model.max_seq_length = 512 \n",
    "print(f\"Max seq length: {model.get_max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "519b92d5-dc8d-4f92-b78f-5c2fefb511bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_desc_embedding_cache={} #dictionary to store 'short_desc':'embedding' for faster computation of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8be8e189-16ce-4c88-942d-ac33d23e8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emb(text):\n",
    "    if text in short_desc_embedding_cache:#check if the embedding for short_desc has previously created \n",
    "            return short_desc_embedding_cache[text]\n",
    "    else: \n",
    "        # Handling missing values\n",
    "        if (text is None) or (text == \"\") or (pd.isna(text)):\n",
    "            short_desc_embedding_cache[text] = np.zeros(512)#save copy to cache\n",
    "            return np.zeros(512)\n",
    "        else:\n",
    "            embedding = model.encode(text)\n",
    "            short_desc_embedding_cache[text] = embedding#save copy to cache\n",
    "            return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23374bf5-357d-4c65-b17c-b40f217ee953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ea7cb932f844d79262fd76676f6422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['embedding'] = df.progress_apply(lambda x: generate_emb(x['short_desc']), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e53a1a-b202-4933-9990-336cec46f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TIME 3:40 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a17056-d8b7-4c4a-883f-7c54f1d9ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_desc_embedding_cache={}#.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f29f74-d7f1-455b-9cae-2fdf9fb4e59c",
   "metadata": {},
   "source": [
    "# Query semantic nearest neighbour for each embedding to identify Semantic/Temporal duplicates \n",
    "- Based on semantic neareast-neighbours search we used cosine similarity to take the most similar subset \n",
    "- On the selected subset we used custom method to remove non-duplicates offers and to assign category to pair as either Semantic or Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a212f6-9c01-49f9-b4a5-b126f6ed3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix indexing\n",
    "df = df.sort_values(['id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b214a2",
   "metadata": {},
   "source": [
    "## Initialize Brute force indexing\n",
    "- Brute force version of searching nearest neighbours in high-dimensional space provided by https://github.com/nmslib/hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1808baa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding batch of 112056 elements\n",
      "Indices built\n"
     ]
    }
   ],
   "source": [
    "# Set the dimensionality of the embeddings to 512 and set the random seed to 42\n",
    "dim = 512\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract the embeddings from the \"embedding\" column of the DataFrame as a list\n",
    "embeddings = df[\"embedding\"].tolist()\n",
    "\n",
    "# Get the number of elements in the dataset\n",
    "num_elements = len(embeddings)\n",
    "\n",
    "# Convert the embeddings to a numpy array\n",
    "data = np.array(embeddings)\n",
    "\n",
    "# Initialize a brute-force index with the cosine similarity metric and the specified dimensionality\n",
    "bf_index = hnswlib.BFIndex(space='cosine', dim=dim)\n",
    "\n",
    "# Initialize the index with the maximum number of elements\n",
    "bf_index.init_index(max_elements=num_elements)\n",
    "\n",
    "# Add the embeddings to the index\n",
    "print(\"Adding batch of %d elements\" % (len(data)))\n",
    "bf_index.add_items(data)\n",
    "\n",
    "# Print a message indicating that the index has been built\n",
    "print(\"Indices built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a8bd6",
   "metadata": {},
   "source": [
    "## Query NN for each embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e1fff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard(set1, set2):\n",
    "    \"\"\"Function to compute jaccard similarity based on words splitted by space\"\"\"\n",
    "    if(set1 is None) or (set1 is None):\n",
    "        return 0.0\n",
    "    try:\n",
    "        set1 = set(set1.split())\n",
    "        set2 = set(set2.split())\n",
    "        return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "queryResult={\n",
    "    'Left':[],#offer ID (smaller id of two)\n",
    "    'Right' :[],#offer ID (bigger id of two)\n",
    "    'cosSim': [] # List to store cosine similarity scores between offer embedding based on 'short_desc'\n",
    "    \n",
    "}\n",
    "\n",
    "k = 43 # Number of nearest neighbors to query\n",
    "total = len(df) # Total number of offers in the dataset\n",
    "\n",
    "# Loop over each offer in the dataset\n",
    "for index, row in tqdm(df.iterrows(), total=total):\n",
    "    # Find k nearest neighbors using brute-force search (k-NN)\n",
    "    labels_bf, distances_bf = bf_index.knn_query(row['embedding'], k)\n",
    "    \n",
    "    # Get embedding and 'description' of current offer\n",
    "    embedding1 = df.iloc[index, -1]\n",
    "    embedding1_desc = df.iloc[index, 2]\n",
    "    \n",
    "    # Loop over each of the k nearest neighbors\n",
    "    for i in range(len(labels_bf[0])):\n",
    "        # Get IDs of current offer and its neighbor\n",
    "        offerID = index + 1\n",
    "        neighbourOfferID = int(labels_bf[0][i] + 1)\n",
    "        \n",
    "        # Store IDs of offers in queryResult in a way that Left < Right\n",
    "        if offerID > neighbourOfferID:\n",
    "            queryResult['Left'].append(neighbourOfferID)\n",
    "            queryResult['Right'].append(offerID)\n",
    "        elif offerID < neighbourOfferID:\n",
    "            queryResult['Left'].append(offerID)\n",
    "            queryResult['Right'].append(neighbourOfferID)\n",
    "        else:\n",
    "            #skip if two IDs are the same\n",
    "            continue\n",
    "        \n",
    "        # Get embedding and description of current neighbour\n",
    "        embedding2 = df.iloc[labels_bf[0][i], -1]\n",
    "        embedding2_desc = df.iloc[labels_bf[0][i], 2]\n",
    "        \n",
    "        \n",
    "        # Compute cosine similarity score between offer embedding based on 'short_desc'\n",
    "        cos_sim = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "        queryResult['cosSim'].append(cos_sim)\n",
    "#TIME 40 mins     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f6bc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## create df with nearest-neighbours pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7141ae84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nn = pd.DataFrame(queryResult)\n",
    "df_nn=df_nn.drop_duplicates(subset=['Left','Right'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88c038-8bcb-45e0-bcdf-240c51a3df1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "488e930d-9fab-48d4-9531-367caa3d2836",
   "metadata": {},
   "source": [
    "## Assign category to pairs from semantic similarity search\n",
    "- To ensure title semantic similarity we used the same model as previously https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2 with maximum number of input tokens (50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a5d39-30ac-4d6f-a488-0f047d4be875",
   "metadata": {},
   "source": [
    "### Partial duplicate identification\n",
    "- We use LSH to identify parent offer (if exists) that contains all details (is extension of child)\n",
    "- Then the child offer is a offer that is missing some information but does not have additional characteristics\n",
    "- Every child offer is potential partial duplicate localized in subset of semantic duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ce43a1a8-dcbd-4136-804a-4c5263c715fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard(set1, set2):\n",
    "    \"\"\"Function to compute jaccard similarity based on words splitted by space\"\"\"\n",
    "    if(set1 is None) or (set1 is None):\n",
    "        return 0.0\n",
    "    try:\n",
    "        set1 = set(set1.split())\n",
    "        set2 = set(set2.split())\n",
    "        return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "def nMissing(text1,text2):\n",
    "    \"\"\"Method to measure how many more words are in text2\"\"\"\n",
    "    text1=set(text1.split())\n",
    "    text2=set(text2.split())\n",
    "    isntext1nottext2 = text1.difference(text2)\n",
    "    isntext2nottext1 = text2.difference(text1)\n",
    "    if((len(isntext1nottext2) == 0) and (len(isntext2nottext1)>0)):\n",
    "        #only interesting case when the text1(child offer) contains no more information then the parent have \n",
    "        return len(isntext2nottext1)\n",
    "        \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# Define number of permutations and size of n-grams\n",
    "permutations = 128\n",
    "n_gram_size = 1\n",
    "\n",
    "def get_forest(data, perms):\n",
    "    minhash = [] # List to store MinHash objects for each offer's short description\n",
    "    \n",
    "    # Loop over each offer's short description in the dataset\n",
    "    for text in data['description']:\n",
    "        tokens = text.split(\" \") # Split text into tokens (words)\n",
    "        ngrams = [' '.join(tokens[i:i+n_gram_size]) for i in range(len(tokens)-n_gram_size-1)] # Generate n-grams of specified size\n",
    "        m = MinHash(num_perm=perms) # Create MinHash object with specified number of permutations\n",
    "        for s in ngrams:\n",
    "            m.update(s.encode('utf8')) # Update MinHash object with each n-gram\n",
    "        minhash.append(m) # Add MinHash object to list\n",
    "        \n",
    "    forest = MinHashLSHForest(num_perm=perms) # Create MinHash LSH Forest object with specified number of permutations\n",
    "    \n",
    "    # Add each offer's MinHash object to the LSH Forest\n",
    "    for i, m in enumerate(minhash):\n",
    "        forest.add(i, m)\n",
    "        \n",
    "    forest.index() # Index the MinHash LSH Forest\n",
    "    \n",
    "    return forest # Return the MinHash LSH Forest object\n",
    "\n",
    "\n",
    "def predict(text, database, perms, num_results, forest):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokens =text.split(\" \") # Split text into tokens (words)\n",
    "    ngrams = [' '.join(tokens[i:i+n_gram_size]) for i in range(len(tokens)-n_gram_size-1)] # Generate n-grams of specified size from tokens\n",
    "    \n",
    "    m = MinHash(num_perm=perms) # Create MinHash object with specified number of permutations\n",
    "    for s in ngrams:\n",
    "        m.update(s.encode('utf8')) # Update MinHash object with each n-gram\n",
    "    \n",
    "    idx_array = np.array(forest.query(m, num_results)) # Query the MinHash LSH Forest for similar offers and return an array of their indices in the database\n",
    "    \n",
    "    if len(idx_array) == 0:\n",
    "        return None # Return None if no similar offers were found\n",
    "    \n",
    "    # Compute Jaccard similarity between the query and each similar offer's 'short_desc_loc' and store in a list of tuples with offer IDs\n",
    "    jaccard_results = []\n",
    "    for idx in idx_array:\n",
    "        jaccard_sim = compute_jaccard(database.iloc[idx]['description'], text) # Compute Jaccard similarity between the query and the similar offer's short description\n",
    "        offer_id = database.iloc[idx]['id'] # Get the ID of the similar offer\n",
    "        jaccard_results.append((offer_id, jaccard_sim)) # Append a tuple of the offer ID and its Jaccard similarity to the list\n",
    "        \n",
    "    jaccard_results = sorted(jaccard_results, key=lambda x: x[1], reverse=True) # Sort the list of similar offers by decreasing Jaccard similarity\n",
    "    \n",
    "    return jaccard_results # Return the list of similar offers with their IDs and Jaccard similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc80704a-48cf-4dee-a040-aa82b4c3124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc=df.dropna(subset=['description'])\n",
    "df_desc=df_desc.reset_index(drop=True)\n",
    "forest = get_forest(df_desc, permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5e1da-c835-41f4-bd9c-19381aef7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "possiblePartial=[]\n",
    "# Define the number of recommended(closest offer to query) offers\n",
    "num_recommendations = 50\n",
    "\n",
    "# Loop through each row of the dataframe\n",
    "#check if offer have parent description\n",
    "for index, row in tqdm(df_desc.iterrows(),total=df_desc.shape[0]):\n",
    "    \n",
    "    \n",
    "    # Extract the query text and generate a set of predicted offers based on the LSH forest\n",
    "    query = row['description']\n",
    "    result = predict(query, df_desc, permutations, num_recommendations, forest)\n",
    "    \n",
    "    # Extract the ID and retrieval date of the current offer\n",
    "    offerIDLeft = row['id']\n",
    "\n",
    "\n",
    "    # Loop through the predicted offers and compare them to the current offer\n",
    "    for res in result:\n",
    "        \n",
    "        # Extract the Jaccard similarity and ID of the predicted offer\n",
    "        jaccard_sim = res[1]\n",
    "        offerIDRight = res[0]\n",
    "        \n",
    "        rightDesc = df_desc[df_desc['id']==offerIDRight].reset_index(drop=True)['description'][0]\n",
    "        if jaccard_sim<0.2:#pairs with lower jaccard will have to much differences\n",
    "            continue\n",
    "        else:\n",
    "            nMiss = nMissing(query,rightDesc)#number of missing words in query\n",
    "            if (nMiss<20) and (nMiss>0):#if its about the lenght of common job characteristics then assign offer id partial class\n",
    "                possiblePartial.append(\n",
    "                    {'offerIDPartial':offerIDLeft,\n",
    "                        'offerIDExtension' :offerIDRight,\n",
    "                        'type' :\"PARTIAL\",\n",
    "                         'nMissing':nMiss\n",
    "                        })\n",
    "                \n",
    "\n",
    "#TIME 1:40 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad940468-010e-47f1-92be-cf282b60732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partial = pd.DataFrame(possiblePartial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2338aa62-29b4-48ad-8364-945aad85d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save identified partial offer's id to list where there is at least two words missing\n",
    "idPartial = df_partial[df_partial['nMissing']>=2]['offerIDPartial'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3a120-8f6c-4632-a543-5c2d9580a05a",
   "metadata": {},
   "source": [
    "#### Initialize methods to assign category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee9eb979-137d-4f0e-92d0-76683f821daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_title = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "model_title.max_seq_length = 50\n",
    "emb_cache = {}#dictionary to store 'title':'title_embedding'\n",
    "\n",
    "\n",
    "def get_class_cos_1(row, cos_title_threshold_temp=0.65, cos_title_threshold_sem=0.58): \n",
    "    offer1 = df[df['id']==row['Left']].reset_index(drop=True)\n",
    "    offer2 = df[df['id']==row['Right']].reset_index(drop=True)\n",
    "    title1 = offer1['title'][0]\n",
    "    title2 = offer2['title'][0]\n",
    "    \n",
    "    same_or_nan_num = 0\n",
    "    for feature in ['location', 'country_id', 'company_name']:\n",
    "        if ((offer1[feature][0] == offer2[feature][0]) or (offer1[feature][0] is np.nan and offer2[feature][0] is np.nan)):\n",
    "            same_or_nan_num+=1\n",
    "        \n",
    "    if same_or_nan_num==3:\n",
    "        if title1 in emb_cache:\n",
    "            emb1 = emb_cache.get(title1)\n",
    "        else:\n",
    "            emb1 = model_title.encode(title1)\n",
    "            emb_cache[title1] = emb1\n",
    "        if title2 in emb_cache:\n",
    "            emb2 = emb_cache.get(title2)\n",
    "        else:\n",
    "            emb2 = model_title.encode(title2)\n",
    "            emb_cache[title2] = emb2\n",
    "            \n",
    "        if offer1['retrieval_date'][0] == offer2['retrieval_date'][0]:\n",
    "            if ((offer1['description'][0]==offer2['description'][0]) and (title1==title2)):\n",
    "                return 'FULL'\n",
    "            else:\n",
    "                if np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)) >= cos_title_threshold_sem:\n",
    "                    return 'SEMANTIC'\n",
    "                else:\n",
    "                    return 'NON-DUPLICATE'\n",
    "        else:\n",
    "            if np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)) >= cos_title_threshold_temp:\n",
    "                return 'TEMPORAL'\n",
    "            else:\n",
    "                return 'NON-DUPLICATE'\n",
    "    else:\n",
    "        return 'NON-DUPLICATE'\n",
    "\n",
    "\n",
    "def get_class(row, cos_title_threshold_temp=0.65, cos_title_threshold_sem=0.56): \n",
    "    \"\"\"Filter method prepared based on many analysis how to best assign duplicate category\"\"\"\n",
    "    # Get the left and right offers from the DataFrame\n",
    "    offer1 = df[df['id']==row['Left']].reset_index(drop=True)\n",
    "    offer2 = df[df['id']==row['Right']].reset_index(drop=True)\n",
    "    \n",
    "    # Get the titles of the left and right offers\n",
    "    title1 = offer1['title'][0]\n",
    "    title2 = offer2['title'][0]\n",
    "    \n",
    "    # Count the number of features that are the same or both are NaN\n",
    "    same_or_nan_num = 0\n",
    "    for feature in ['location', 'country_id', 'company_name']:\n",
    "        if ((offer1[feature][0] == offer2[feature][0]) or (offer1[feature][0] is np.nan and offer2[feature][0] is np.nan)):\n",
    "            same_or_nan_num += 1\n",
    "        \n",
    "    # If all three features are the same or both are NaN\n",
    "    if same_or_nan_num == 3:\n",
    "        # Compute the embeddings for the left and right offer titles using the model_title\n",
    "        if title1 in emb_cache:\n",
    "            emb1 = emb_cache.get(title1)\n",
    "        else:\n",
    "            emb1 = model_title.encode(title1)\n",
    "            emb_cache[title1] = emb1\n",
    "        if title2 in emb_cache:\n",
    "            emb2 = emb_cache.get(title2)\n",
    "        else:\n",
    "            emb2 = model_title.encode(title2)\n",
    "            emb_cache[title2] = emb2\n",
    "            \n",
    "        # If the retrieval dates are the same\n",
    "        if offer1['retrieval_date'][0] == offer2['retrieval_date'][0]:\n",
    "            # If the descriptions and titles are the same\n",
    "            if ((offer1['description'][0]==offer2['description'][0]) and (title1==title2)):\n",
    "                return 'FULL'\n",
    "            # If the cosine similarity between the embeddings of the titles is above the semantic threshold\n",
    "            else:\n",
    "                if np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)) >= cos_title_threshold_sem:\n",
    "                    if(row['Left'] in idPartial) or(row['Right'] in idPartial):\n",
    "                        #if pair contains id that were identified as potential partial duplicate assign appropriate category\n",
    "                        return \"PARTIAL\"\n",
    "                    else:\n",
    "                        return 'SEMANTIC'\n",
    "                else:\n",
    "                    return 'NON-DUPLICATE'\n",
    "        # If the retrieval dates are different\n",
    "        else:\n",
    "            # If the cosine similarity between the embeddings of the titles is above the temporal threshold\n",
    "            if np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)) >= cos_title_threshold_temp:\n",
    "                return 'TEMPORAL'\n",
    "            else:\n",
    "                return 'NON-DUPLICATE'\n",
    "    # If any of the features are different\n",
    "    else:\n",
    "        return 'NON-DUPLICATE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3041770c-e5b9-43e4-8027-aa5a0215c652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29514d45ff374cb391d7a8531b0b7b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/349503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#separate df for offers with cos similarity ==1\n",
    "#take only semantic duplicate from cosine==1 as by using short_desc temporal duplicate contains many false positives\n",
    "df_nn_cos_1 = df_nn[df_nn['cosSim']==1]\n",
    "#filter only semantic/temporal pairs\n",
    "tqdm.pandas()\n",
    "df_nn_cos_1['Class'] = df_nn_cos_1.progress_apply( lambda x: get_class_cos_1(x),axis=1)\n",
    "df_nn_cos_1 = df_nn_cos_1[df_nn_cos_1['Class'].isin(['SEMANTIC'])]\n",
    "df_nn_cos_1.drop('cosSim',axis=1,inplace=True)\n",
    "#TIME 8 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30992819-c16d-4c8b-aa96-c7cccc09bb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdc77eb42414151a37d20fc3f2334fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3003241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cosine similarity threshold for temporal/semantic pair\n",
    "min_similarity_threshold_semantic = 0.35\n",
    "min_similarity_threshold_temporal = 0.48\n",
    "\n",
    "#reduce pairs of nearest neighbours search \n",
    "df_nn=df_nn[(df_nn['cosSim']>=min_similarity_threshold_semantic)&(df_nn['cosSim']<1)]\n",
    "\n",
    "total_pairs = df_nn.shape[0]\n",
    "\n",
    "Class = [] #list to store assigned category for entire semantic neareast-neighbours dataframe\n",
    "# Loop over each pair in the dataset\n",
    "for index, row in tqdm(df_nn.iterrows(), total=total_pairs):\n",
    "    offer1ID = row['Left']\n",
    "    offer2ID = row['Right']\n",
    "    \n",
    "    offer1Row = df[df['id']==offer1ID].reset_index(drop=True)\n",
    "    offer2Row = df[df['id']==offer2ID].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    if offer1Row.iat[0,6]==offer2Row.iat[0, 6]:#compare offer's retrieval dates\n",
    "        #semantic threshold above include all pairs \n",
    "        Class.append(get_class(row))\n",
    "    else:\n",
    "        #for temporal pairs threshold needs to be check as there are pairs with cosine belowe 0.5\n",
    "        if (row['cosSim']>=min_similarity_threshold_temporal):\n",
    "            Class.append(get_class(row))\n",
    "        else:\n",
    "            Class.append(\"NON-DUPLICATE\")\n",
    "            continue\n",
    "    #TIME 4h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c5df164-cbed-426b-a96d-a66135d88e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn['Class']=Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7791e68-2953-472f-80e4-b318ca51dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn=df_nn[df_nn['Class'].isin(['TEMPORAL','SEMANTIC',\"PARTIAL\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ef2ec53d-8803-4369-9e49-ab9c5a86b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn=df_nn.drop('cosSim',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b6227-5076-48c2-914a-ad5471f53688",
   "metadata": {},
   "source": [
    "## LSH (Locality Sensitive Hashing) to identify partial duplicates\n",
    "- During early submissions we used LSH to identify full duplicates, what we found out was that this approach gave as a chance to discover partial/semantic duplicates with high precision\n",
    "- The approach uses MinHashLSHForest from datasketch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c201cc26-9ad7-4ee1-91c9-db4828d11b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Drop NaN short_desc rows as they are not supported in methods below\n",
    "df_short= df.dropna(subset=['short_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b0d12a5-6009-496f-a36e-b69d63103aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure higher probability of match the content from 'location' column was added to 'short_desc' -> so that 'short_desc_loc' column was made\n",
    "df_short['short_desc_loc'] = df_short.apply(lambda x : x['location']+\" \"+x['short_desc'] if isinstance(x['location'],str)  else x['short_desc'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3175eb64-713c-4f66-90fc-12e272e5ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of permutations and size of n-grams\n",
    "permutations = 128\n",
    "n_gram_size = 5\n",
    "\n",
    "def get_forest(data, perms):\n",
    "    minhash = [] # List to store MinHash objects for each offer's short description\n",
    "    \n",
    "    # Loop over each offer's short description in the dataset\n",
    "    for text in data['short_desc_loc']:\n",
    "        tokens = text.split(\" \") # Split text into tokens (words)\n",
    "        ngrams = [' '.join(tokens[i:i+n_gram_size]) for i in range(len(tokens)-n_gram_size-1)] # Generate n-grams of specified size\n",
    "        m = MinHash(num_perm=perms) # Create MinHash object with specified number of permutations\n",
    "        for s in ngrams:\n",
    "            m.update(s.encode('utf8')) # Update MinHash object with each n-gram\n",
    "        minhash.append(m) # Add MinHash object to list\n",
    "        \n",
    "    forest = MinHashLSHForest(num_perm=perms) # Create MinHash LSH Forest object with specified number of permutations\n",
    "    \n",
    "    # Add each offer's MinHash object to the LSH Forest\n",
    "    for i, m in enumerate(minhash):\n",
    "        forest.add(i, m)\n",
    "        \n",
    "    forest.index() # Index the MinHash LSH Forest\n",
    "    \n",
    "    return forest # Return the MinHash LSH Forest object\n",
    "\n",
    "\n",
    "def predict(text, database, perms, num_results, forest):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokens =text.split(\" \") # Split text into tokens (words)\n",
    "    ngrams = [' '.join(tokens[i:i+n_gram_size]) for i in range(len(tokens)-n_gram_size-1)] # Generate n-grams of specified size from tokens\n",
    "    \n",
    "    m = MinHash(num_perm=perms) # Create MinHash object with specified number of permutations\n",
    "    for s in ngrams:\n",
    "        m.update(s.encode('utf8')) # Update MinHash object with each n-gram\n",
    "    \n",
    "    idx_array = np.array(forest.query(m, num_results)) # Query the MinHash LSH Forest for similar offers and return an array of their indices in the database\n",
    "    \n",
    "    if len(idx_array) == 0:\n",
    "        return None # Return None if no similar offers were found\n",
    "    \n",
    "    # Compute Jaccard similarity between the query and each similar offer's 'short_desc_loc' and store in a list of tuples with offer IDs\n",
    "    jaccard_results = []\n",
    "    for idx in idx_array:\n",
    "        jaccard_sim = compute_jaccard(database.iloc[idx]['short_desc_loc'], text) # Compute Jaccard similarity between the query and the similar offer's short description\n",
    "        offer_id = database.iloc[idx]['id'] # Get the ID of the similar offer\n",
    "        jaccard_results.append((offer_id, jaccard_sim)) # Append a tuple of the offer ID and its Jaccard similarity to the list\n",
    "        \n",
    "    jaccard_results = sorted(jaccard_results, key=lambda x: x[1], reverse=True) # Sort the list of similar offers by decreasing Jaccard similarity\n",
    "    \n",
    "    return jaccard_results # Return the list of similar offers with their IDs and Jaccard similarities\n",
    "\n",
    "def get_class_lsh(IDLeft,IDRight, cos_title_threshold_temp=0.65, cos_title_threshold_sem=0.58): \n",
    "    \"\"\"The same Filter method as above prepared based on many analysis how to best assign duplicate category\n",
    "    It takes different parameters and is used to only check the df with partial/semantic duplicates.\n",
    "    The non-duplicate offers can be filter out if the result is different that 'SEMANTIC' \"\"\"\n",
    "    offer1 = df[df['id']==IDLeft].reset_index(drop=True)\n",
    "    offer2 = df[df['id']==IDRight].reset_index(drop=True)\n",
    "    title1 = offer1['title'][0]\n",
    "    title2 = offer2['title'][0]\n",
    "    \n",
    "    same_or_nan_num = 0\n",
    "    for feature in ['location', 'country_id', 'company_name']:\n",
    "        if ((offer1[feature][0] == offer2[feature][0]) or (offer1[feature][0] is np.nan and offer2[feature][0] is np.nan)):\n",
    "            same_or_nan_num+=1\n",
    "        \n",
    "    if same_or_nan_num==3:\n",
    "        if title1 in emb_cache:\n",
    "            emb1 = emb_cache.get(title1)\n",
    "        else:\n",
    "            emb1 = model_title.encode(title1)\n",
    "            emb_cache[title1] = emb1\n",
    "        if title2 in emb_cache:\n",
    "            emb2 = emb_cache.get(title2)\n",
    "        else:\n",
    "            emb2 = model_title.encode(title2)\n",
    "            emb_cache[title2] = emb2\n",
    "            \n",
    "        if offer1['retrieval_date'][0] == offer2['retrieval_date'][0]:\n",
    "            if ((offer1['description'][0]==offer2['description'][0]) and (title1==title2)):\n",
    "                return 'FULL'\n",
    "            else:\n",
    "                if np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)) >= cos_title_threshold_sem:\n",
    "                    return 'SEMANTIC'\n",
    "                else:\n",
    "                    return 'NON-DUPLICATE'\n",
    "        else:\n",
    "            if np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)) >= cos_title_threshold_temp:\n",
    "                return 'TEMPORAL'\n",
    "            else:\n",
    "                return 'NON-DUPLICATE'\n",
    "    else:\n",
    "        return 'NON-DUPLICATE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed380af-4bca-4948-8ec1-bd7857dd1e9d",
   "metadata": {},
   "source": [
    "### Creation of LSHForest for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b00dfbaa-ac38-4575-8350-0e872e6ba66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cd9b12e7974291833d2fda3dcd61d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a dictionary to hold the forests for each country\n",
    "forest_dict={}\n",
    "\n",
    "# Loop through all unique country IDs in the dataset\n",
    "for country in tqdm(df_short.country_id.unique()):\n",
    "    \n",
    "    # Filter the dataframe to only include offers from the current country\n",
    "    df_country = df_short.query(f\"country_id == '{country}'\").reset_index(drop=True) \n",
    "    \n",
    "    # Generate the forest for the current country's offers\n",
    "    forest_country = get_forest(df_country, permutations)\n",
    "    \n",
    "    # Add the current country's dataframe and forest to the dictionary\n",
    "    forest_dict[f\"{country}\"]={}\n",
    "    forest_dict[f\"{country}\"]['df_country'] = df_country\n",
    "    forest_dict[f\"{country}\"]['forest'] = forest_country\n",
    "    \n",
    "#TIME 4 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e82db9-8a59-414d-b745-a268c5686f8b",
   "metadata": {},
   "source": [
    "## Predict partial/semantic duplicates based on query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "623e1b7d-e872-42be-831b-85503514ec6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022baed6354e4f7994f9e2647e9233b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize an empty list to store results\n",
    "LSHResult=[]\n",
    "# Define the number of recommended(closest offer to query) offers\n",
    "num_recommendations = 100\n",
    "\n",
    "# Loop through each row of the dataframe\n",
    "for index, row in tqdm(df_short.iterrows(),total=df_short.shape[0]):\n",
    "    # Retrieve the LSH forest and country-specific dataframe for the current row\n",
    "    countryForest = forest_dict[row['country_id']]['forest']\n",
    "    df_country = forest_dict[row['country_id']]['df_country']\n",
    "    \n",
    "    # Extract the query text and generate a set of predicted offers based on the LSH forest\n",
    "    query = row['short_desc_loc']\n",
    "    result = predict(query, df_country, permutations, num_recommendations, countryForest)\n",
    "    \n",
    "    # Extract the ID and retrieval date of the current offer\n",
    "    offerIDLeft = row['id']\n",
    "    ret_date_offerLeft= row['retrieval_date']\n",
    "\n",
    "    # Loop through the predicted offers and compare them to the current offer\n",
    "    for res in result:\n",
    "        \n",
    "        # Extract the Jaccard similarity and ID of the predicted offer\n",
    "        jaccard_sim = res[1]\n",
    "        offerIDRight = res[0]\n",
    "        \n",
    "        # Extract the row of the predicted offer from the country-specific dataframe\n",
    "        row_to_check = df_country.query(f\"id == {offerIDRight}\")\n",
    "        \n",
    "        # If the predicted offer is the same as the current offer, skip it\n",
    "        if (offerIDLeft==offerIDRight):\n",
    "            continue\n",
    "        \n",
    "        # If the predicted offer has a Jaccard similarity between 0.6 and 0.8, and the same retrieval date as the current offer, add it to the LSHResult list\n",
    "        elif((jaccard_sim <0.8) and(jaccard_sim >=0.6)):\n",
    "            ret_date_offerRight = row_to_check['retrieval_date'].iloc[0]\n",
    "            \n",
    "            if(ret_date_offerLeft==ret_date_offerRight):#THE SAME DATE\n",
    "                #set valid index\n",
    "                if (offerIDLeft<offerIDRight):\n",
    "                    LSHResult.append(\n",
    "                    {'offerIDLeft':offerIDLeft,\n",
    "                        'offerIDRight' :offerIDRight,\n",
    "                        'type' :\"SEMANTIC\",\n",
    "                         'Class':get_class_lsh(offerIDLeft,offerIDRight)\n",
    "                        })\n",
    "                else:\n",
    "                    LSHResult.append(\n",
    "                    {'offerIDLeft':offerIDRight,\n",
    "                        'offerIDRight' :offerIDLeft,\n",
    "                        'type' :\"SEMANTIC\",\n",
    "                         'Class':get_class_lsh(offerIDLeft,offerIDRight)\n",
    "                        })\n",
    "        \n",
    "        # If the predicted offer has a Jaccard similarity between 0.8 and 1, and the same retrieval date as the current offer, add it to the LSHResult list\n",
    "        elif((jaccard_sim >0.8) and(jaccard_sim <1)):\n",
    "            ret_date_offerRight = row_to_check['retrieval_date'].iloc[0]\n",
    "            \n",
    "            if(ret_date_offerLeft==ret_date_offerRight):#THE SAME DATE\n",
    "                #set valid index\n",
    "                if (offerIDLeft<offerIDRight):\n",
    "                    LSHResult.append(\n",
    "                    {'offerIDLeft':offerIDLeft,\n",
    "                        'offerIDRight' :offerIDRight,\n",
    "                        'type' :\"PARTIAL\",\n",
    "                         'Class':get_class_lsh(offerIDLeft,offerIDRight)\n",
    "                        })\n",
    "                else:\n",
    "                    LSHResult.append(\n",
    "                    {'offerIDLeft':offerIDRight,\n",
    "                        'offerIDRight' :offerIDLeft,\n",
    "                        'type' :\"PARTIAL\",\n",
    "                         'Class':get_class_lsh(offerIDLeft,offerIDRight)\n",
    "                        })\n",
    "        \n",
    "        # If the predicted offer has a Jaccard similarity less than 0.6 or retrieval date different from current offer, skip it\n",
    "        else:\n",
    "            pass\n",
    "#TIME 40 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd0c8579-9dc8-4a06-918c-09e726e98a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store result inside separate df\n",
    "df_lsh_partial_semantic = pd.DataFrame(LSHResult)\n",
    "#assign class based on previous type and Class from get_class method; duplicates with assign SEMANTIC are the ones to save\n",
    "df_lsh_partial_semantic['Class']=df_lsh_partial_semantic.apply(lambda x: x['type'] if x['Class']=='SEMANTIC' else 'NON-DUPLICATE',axis=1)\n",
    "#filter only duplicate pairs\n",
    "df_lsh_partial_semantic=df_lsh_partial_semantic[df_lsh_partial_semantic['Class'].isin(['SEMANTIC','PARTIAL'])]\n",
    "#change columns names to match other dfs\n",
    "df_lsh_partial_semantic=df_lsh_partial_semantic.rename(columns={'offerIDLeft':'Left','offerIDRight':'Right'})\n",
    "#remove redudant column\n",
    "df_lsh_partial_semantic.drop(['type'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3431b-c94e-476e-ab38-4a3adf68cfc4",
   "metadata": {},
   "source": [
    "# Identification of full/temporal duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf83ee4c-425e-49f2-8fd9-a7ee32cbd25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].str.lower()\n",
    "df['title'] = df['title'].str.lower()\n",
    "df['company_name'] = df['company_name'].str.lower()\n",
    "df['location'] = df['location'].str.lower()\n",
    "# Count the number of occurrences of each unique 'title' value in the DataFrame 'df'\n",
    "title_count = df['title'].value_counts()\n",
    "# Keep only the values in 'title_count' that appear more than once\n",
    "title_count = title_count[title_count>1]\n",
    "\n",
    "# Count the number of occurrences of each unique 'description' value in the DataFrame 'df'\n",
    "desc_count = df['description'].value_counts()\n",
    "# Keep only the values in 'desc_count' that appear more than once\n",
    "desc_count = desc_count[desc_count>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f32bbab6-b522-413e-87f9-50e0ea83f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows from df that have a 'title' value in title_count and a 'description' value in desc_count\n",
    "df_dup = df[(df['title'].isin(title_count.index)) & (df['description'].isin(desc_count.index))]\n",
    "# Reset the index of df_dup to start from 0 and drop the old index\n",
    "df_dup = df_dup.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5efe05a3-7f6b-489c-8ab5-945c95a1e584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0447545954584c6c850130a25e7d78d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54081 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize empty lists to store information about duplicates\n",
    "Left = []\n",
    "Right = []\n",
    "Class = []\n",
    "\n",
    "# Use tqdm to display a progress bar during the loop\n",
    "with tqdm(total=len(df_dup)) as pbar:\n",
    "    # Iterate over the rows of df_dup\n",
    "    for row in df_dup.iterrows():\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "        # Select the offer from the current row\n",
    "        offer = row[1]\n",
    "        # Find all offers in df with the same title as the current offer\n",
    "        to_check = df[df['title']==offer['title']]\n",
    "        # Iterate over these offers to find duplicates\n",
    "        for row in to_check.iterrows():\n",
    "            # Select the offer to check for duplication\n",
    "            offer_check = row[1]\n",
    "            # Check if the offer and offer_check have identical title, description, and different id\n",
    "            if offer['description']==offer_check['description']\\\n",
    "            and offer['title']==offer_check['title']\\\n",
    "            and offer['id']!=offer_check['id']:\n",
    "                # If duplicates are found, store their ids in Left and Right lists\n",
    "                Left.append(offer['id'])\n",
    "                Right.append(offer_check['id'])\n",
    "                # Classify the type of duplicate based on whether they have the same retrieval date or not\n",
    "                if offer['retrieval_date']==offer_check['retrieval_date']:\n",
    "                    Class.append(\"FULL\")\n",
    "                else:\n",
    "                    Class.append(\"TEMPORAL\")\n",
    "#TIME 6 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a713a25-fec2-4647-8a09-eb0f32872854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_higher_on_the_right(df_uncharted):\n",
    "    \"\"\"Method to change the index inside df; id1<id2\"\"\"\n",
    "    df = df_uncharted.copy(deep=True)\n",
    "    i = 0\n",
    "    with tqdm(total=len(df)) as pbar:\n",
    "        for row in df.iterrows():\n",
    "            pbar.update(1)\n",
    "            if row[1]['Right'] < row[1]['Left']:\n",
    "                left = row[1]['Right']\n",
    "                right = row[1]['Left']\n",
    "                df.iat[i, 0] = left\n",
    "                df.iat[i, 1] = right\n",
    "            i+=1\n",
    "    return df\n",
    "\n",
    "def is_right(df):\n",
    "    '''Test method to check duplicates.csv assumptions'''\n",
    "    left = df['Left'].to_list()\n",
    "    right = df['Right'].to_list()\n",
    "    for i in range(len(left)):\n",
    "        if left[i] >= right[i]:\n",
    "            raise Exception(f\"On index {i} left value is greater or equal than right.\")\n",
    "            \n",
    "    if df[['Left', 'Right']].duplicated().sum()>0:\n",
    "        raise Exception(f\"There is duplicated rows.\")\n",
    "    print(\"Everything ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "94bcdfe7-b266-4817-95ef-3a4e2be840d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42581926f61044029291cbab620bd5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/714936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_full_temporal = pd.DataFrame({\"Left\": Left, \"Right\": Right, \"Class\": Class})\n",
    "df_full_temporal = set_higher_on_the_right(df_full_temporal)\n",
    "#TIME 6 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dddea483-0812-4d28-9e0c-e667b16f52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_temporal.drop_duplicates(subset=['Left', 'Right'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "67ecdfbd-ab06-4215-9681-cb8115a9b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign 'PARTIAL' category for only 'FULL' pairs if any index from pair is possiblePartial (idPartial)\n",
    "df_full_temporal['Class'] = df_full_temporal.apply(lambda x : 'PARTIAL' if (((x['Left'] in idPartial) or (x['Right'] in idPartial)) and (x['Class'] in ['FULL'])) else x['Class'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f5d72-caf1-407e-97b0-c32f6e558129",
   "metadata": {},
   "source": [
    "## Create duplicates.csv from many df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f372676-d6a3-4cdf-a39e-be17876041a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "793d4cbc-26f1-4173-b5d2-ddcd166a6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the order of the dfs is important as it mirrors the confidence of indetified pairs\n",
    "df_result = pd.concat([df_full_temporal ,df_lsh_partial_semantic,df_nn,df_nn_cos_1], axis=0)\n",
    "df_result.drop_duplicates(subset=['Left', 'Right'], keep='first', inplace=True)\n",
    "df_result.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1fbe7c9a-7403-4fe5-9927-dee66095ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('./duplicates.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
